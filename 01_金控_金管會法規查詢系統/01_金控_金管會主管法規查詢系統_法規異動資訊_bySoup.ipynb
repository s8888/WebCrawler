{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Project Name: 金管會主管法規查詢系統_法規異動資訊\n",
    "CTM Name    : \n",
    "Crawl Cycle : Daily\n",
    "Main Website: http://law.fsc.gov.tw/law\n",
    "Description : 針對金管會主管法規查詢系統，每日取得最新法規異動資訊\n",
    "Author      : 林竣昇\n",
    "Update Date : 2018/12/18\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import header\n",
    "import logging\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TempPath = \"./Temp/\"  # browser file\n",
    "FinalPath = \"./Result/\" # project file\n",
    "lastResultPath = \"./CrawlList/\"\n",
    "lastResultName = \"lastResult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailFromContent(soup, content, tempMap, link):\n",
    "    if link.find(\"law.fsc.gov.tw\") != -1:\n",
    "        # 發文字號\n",
    "        serialNumber = soup.select(\"#ctl00_cp_content_trODWord td\")\n",
    "        if len(serialNumber) == 0:\n",
    "            serialNumber = soup.select(\"tr:nth-of-type(3) td\")\n",
    "        serialNumber = serialNumber[0].text[:serialNumber[0].text.find(\"號\") + 1].strip()\n",
    "\n",
    "        # 發文日期\n",
    "        date = soup.select(\"#ctl00_cp_content_trAnnDate td\")\n",
    "        if len(date) == 0:\n",
    "            date = soup.select(\"td.text-middle\")[0].text.strip()\n",
    "            date = date[:date.find(\"\\r\")]\n",
    "        else: \n",
    "            date = date[0].text.strip()\n",
    "\n",
    "        # 相關法條\n",
    "        strPos = content.find(\"據：\")\n",
    "        if strPos != -1:\n",
    "            strPos = strPos + 2\n",
    "            endPos = strPos + content[strPos:].find(\"：\")\n",
    "            stopPos = strPos + content[strPos:endPos].rfind(\"\\r\")\n",
    "            relatedLaw = content[strPos:stopPos].strip()\n",
    "        else:\n",
    "            relatedLaw = \"\"\n",
    "            \n",
    "    elif link.find(\"law.moj.gov.tw\"):\n",
    "        # 發文字號\n",
    "        strPos = content.find(\"日\")\n",
    "        if strPos != -1:\n",
    "            strPos = strPos + 1\n",
    "            endPos = content.find(\"號\") + 1\n",
    "            serialNumber = content[strPos:endPos].strip()\n",
    "\n",
    "        # 發文日期\n",
    "        date = soup.select(\"#lbDate\")[0].text.strip()\n",
    "\n",
    "        # 相關法條\n",
    "        relatedLaw = \"\"        \n",
    "        \n",
    "    tempMap['發文字號'] = serialNumber\n",
    "    tempMap['發文日期'] = date\n",
    "    tempMap['相關法條'] = relatedLaw\n",
    "    \n",
    "    return tempMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPdfInsideWebsite(link, df, FinalPath):\n",
    "    try:\n",
    "        soup = request2soup(link)\n",
    "        # 主旨\n",
    "        title = soup.select(\".Block h3\")[0].text.strip()\n",
    "        # 附件\n",
    "        attachment = soup.select(\".embed-responsive-item\")[0].get(\"src\")\n",
    "        if attachment.find(\"https\") == -1:\n",
    "            attachment = \"https://gazette.nat.gov.tw\" + attachment\n",
    "\n",
    "        df = df.append({\"標題\" : title, \n",
    "                        \"附件\" : title}, \n",
    "                       ignore_index = True)\n",
    "\n",
    "        # 建立資料夾\n",
    "        target = FinalPath + \"/\" + title[:30].strip()\n",
    "\n",
    "        # 若目錄不存在，建立目錄\n",
    "        if not os.path.isdir(target):\n",
    "            os.makedirs(target)\n",
    "    \n",
    "        # 下載附件\n",
    "        response = requests.get(attachment, stream = \"TRUE\")\n",
    "        with open(target + \"/\" + title[:30] + \".pdf\", \"wb\") as file:\n",
    "            for data in response.iter_content():\n",
    "                file.write(data)\n",
    "                \n",
    "        print(\"爬取成功\")\n",
    "    except:\n",
    "        logging.error(\"爬取內文失敗\")\n",
    "        logging.error(\"失敗連結：\" + link)\n",
    "        traceback.print_exc()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsingDetail(df, FinalPath):\n",
    "\n",
    "    df_detail = pd.DataFrame(columns = [\"標題\", \"全文內容\", \"發文字號\", \"發文日期\", \"相關法條\", \"附件\"])\n",
    "\n",
    "    for link in df[\"網頁連結\"]:\n",
    "        try:\n",
    "            print(\"爬取網址：\" + link)\n",
    "            \n",
    "            linkSplit = link.split(\"=\")[-1]\n",
    "\n",
    "            # Case1: 內嵌 PDF\n",
    "            if \"detailLog\" == linkSplit:\n",
    "                df_detail = getPdfInsideWebsite(link, df_detail, FinalPath)\n",
    "\n",
    "            else:\n",
    "                # 內容連結\n",
    "                soup = request2soup(link)\n",
    "                subLink = soup.select(\"#ctl00_cp_content_hlkAnnTitle\")\n",
    "                \n",
    "                # Case2: 表格內嵌 PDF\n",
    "                if len(subLink) > 0:\n",
    "                    subLink = subLink[0].get(\"href\")\n",
    "                    df_detail = getPdfInsideWebsite(subLink, df_detail, FinalPath)\n",
    "\n",
    "                # Case3: 表格板\n",
    "                else:\n",
    "                    try:\n",
    "                        if link.find(\"law.fsc.gov.tw\") != -1:\n",
    "                            # 主旨\n",
    "                            title = soup.select(\"h2\")[0].text.strip()\n",
    "                            # 全文內容\n",
    "                            content = soup.select(\".text-con\")[0].text.strip()\n",
    "                            # 附件\n",
    "                            attachments = soup.select(\"#ctl00_cp_content_trAnnFiles02 td a\") # n 個附件\n",
    "                        \n",
    "                        elif link.find(\"law.moj.gov.tw\") != -1:\n",
    "                            # 主旨\n",
    "                            title = soup.select(\"#lbAbstract\")[0].text.strip()\n",
    "                            # 全文內容\n",
    "                            content = soup.select(\"pre\")[0].text.strip()\n",
    "                            # 附件\n",
    "                            attachments = []\n",
    "                        \n",
    "                        tempMap = {\"標題\" : title, \n",
    "                                   \"全文內容\" : content, \n",
    "                                   \"附件\" : \", \".join(str(e.text) for e in attachments)}\n",
    "\n",
    "                        tempMap = getDetailFromContent(soup, content, tempMap, link)\n",
    "                        \n",
    "                        df_detail = df_detail.append(tempMap, ignore_index = True)\n",
    "                        \n",
    "                        if len(attachments) != 0:\n",
    "                            target = FinalPath + \"/\" + title[:30].strip() # 資料夾檔名取 title 前 30 \n",
    "\n",
    "                            # 若目錄不存在，建立目錄\n",
    "                            if not os.path.isdir(target):\n",
    "                                os.makedirs(target)\n",
    "\n",
    "                            # 下載附件\n",
    "                            for attach in attachments:\n",
    "                                response = requests.get(url + \"/\" + attach.get(\"href\"), stream = \"TRUE\")\n",
    "                                fileName = attach.text\n",
    "                                endLoc = fileName.rfind(\".\") # 檔名結尾位置\n",
    "                                extName = fileName[endLoc:]  # 副檔名\n",
    "                                fileName = fileName[:endLoc].strip() # 檔名\n",
    "                                fileName = fileName[:30]     # 截短檔名\n",
    "\n",
    "                                with open(target + \"/\" + fileName + extName, \"wb\") as file:\n",
    "                                    for data in response.iter_content():\n",
    "                                        file.write(data)\n",
    "                                        \n",
    "                        print(\"爬取成功\")\n",
    "                    except:\n",
    "                        logging.error(\"爬取內文失敗\")\n",
    "                        logging.error(\"失敗連結：\" + link)\n",
    "                        traceback.print_exc()\n",
    "                        \n",
    "        except:\n",
    "            logging.error(\"爬取內文失敗\")\n",
    "            traceback.print_exc()\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return df_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputCsv(df, fileName, path):\n",
    "    # 若目錄不存在，建立目錄\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    df.to_csv(path + fileName + \".csv\", index = False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsingTitle(soup, checkRange):\n",
    "\n",
    "    try:\n",
    "        # 取得上次爬網結果\n",
    "        if os.path.isfile(lastResultPath + lastResultName):\n",
    "            lastResult = pd.read_csv(lastResultPath + lastResultName)\n",
    "        else:\n",
    "            lastResult = pd.DataFrame()\n",
    "        \n",
    "        # 爬網日期區間為一個禮拜\n",
    "        endDate = datetime.date.today()\n",
    "        strDate = endDate - datetime.timedelta(days = checkRange)\n",
    "        \n",
    "        dates = []\n",
    "        titles = []\n",
    "        links = []\n",
    "\n",
    "        totalPages = int(soup.select(\".pageinfo\")[0].text.split(\"\\r\\n\")[4].strip()) # 總頁數\n",
    "        pageCounts = int(len(soup.select(\"td\")) / 4) # 每頁筆數\n",
    "        nowPage = 1\n",
    "        ending = False # 是否結束主旨爬網\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            for idx in range(pageCounts):\n",
    "                try:\n",
    "                    nowPage += 1\n",
    "                    date = soup.select(\"td\")[idx * 4 + 1].text.strip()\n",
    "                    dateDT = date.split(\".\")\n",
    "                    dateDT = datetime.date(int(dateDT[0]) + 1911, int(dateDT[1]), int(dateDT[2])) # 轉換成西元年\n",
    "                    \n",
    "                    # 若發文日期小於開始日期, 則結束爬取主旨\n",
    "                    if strDate > dateDT:\n",
    "                        ending = True\n",
    "                        break\n",
    "                    elif dateDT > endDate:\n",
    "                        continue\n",
    "                    \n",
    "                    # 去除前段文字才會與第二層 title 相符\n",
    "                    title = soup.select(\"td a\")[idx].text.strip()\n",
    "                    if (\"金融監督管理委員會令：\" in title) | (\"金融監督管理委員會公告：\" in title):\n",
    "                        title = title.split(\"：\")[1]\n",
    "\n",
    "                    dates.append(date)\n",
    "                    titles.append(title)\n",
    "                    link = soup.select(\"td a\")[idx].get(\"href\")\n",
    "                    if link.find(\"http\") == -1:\n",
    "                        link = url + \"/\" + link\n",
    "                    links.append(link)   \n",
    "\n",
    "                except:\n",
    "                    logging.error(\"爬取第 %s 頁第 %s 筆主旨發生錯誤\" %(nowPage, idx + 1))\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            # 若結束爬取主旨, 停止爬取剩下的 pages\n",
    "            if ending:\n",
    "                break \n",
    "                \n",
    "            nowPage += 1\n",
    "            if nowPage > totalPages:\n",
    "                break\n",
    "            soup = request2soup(url + \"/?&page=%s\" %(nowPage))  \n",
    "            \n",
    "        nowDates = [str(endDate.year - 1911) + \".\" + str(endDate.month) + \".\" + str(endDate.day)] * len(dates)\n",
    "\n",
    "        d = {\"爬文日期\" : nowDates, \"發文日期\" : dates, \"標題\" : titles, \"網頁連結\" : links}\n",
    "        df = pd.DataFrame(data = d, columns = [\"爬文日期\", \"發文日期\", \"標題\", \"網頁連結\"])\n",
    "\n",
    "        # 將這次爬網儲存以便下次爬網比對\n",
    "        outputCsv(df, lastResultName, lastResultPath)\n",
    "\n",
    "        if not lastResult.empty:\n",
    "            # 若與上次發文日期和標題相同，則刪除此筆資料\n",
    "            for i in range(len(df)):\n",
    "                for j in range(len(lastResult)):\n",
    "                    if (df[\"發文日期\"][i] == lastResult[\"發文日期\"][j]) & (df[\"標題\"][i] == lastResult[\"標題\"][j]): \n",
    "                        df.drop(i, inplace = True)\n",
    "                        break\n",
    "                        \n",
    "        if len(df) == 0:\n",
    "            logging.critical(\"%s 至 %s 間無資料更新\" %(strDate, endDate))\n",
    "            return pd.DataFrame(columns = [\"爬文日期\", \"發文日期\", \"標題\", \"網頁連結\"])\n",
    "        \n",
    "        return df\n",
    "    except:\n",
    "        logging.error(\"爬取主旨列表失敗\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame(columns = [\"爬文日期\", \"發文日期\", \"標題\", \"網頁連結\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request2soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"utf-8\"\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\", from_encoding = \"utf-8\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, checkRange = 7):\n",
    "    \n",
    "    logging.critical(\"\\n\")\n",
    "    logging.critical(\"爬網開始......\")\n",
    "    logging.critical(\"目標網址：\" + url)\n",
    "    \n",
    "    strTime = datetime.datetime.now()\n",
    "    logging.critical(\"開始時間：\" + strTime.strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "\n",
    "    try:\n",
    "        soup = request2soup(url)\n",
    "        \n",
    "        df_1 = parsingTitle(soup, checkRange)\n",
    "        if len(df_1) != 0:\n",
    "            outputCsv(df_1, \"第一層結果\", FinalPath)\n",
    "            \n",
    "            df_2 = parsingDetail(df_1, FinalPath)\n",
    "            outputCsv(df_2, \"第二層結果\", FinalPath)\n",
    "    except:\n",
    "        logging.error(\"執行爬網作業失敗\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    endTime = datetime.datetime.now()\n",
    "    logging.critical(\"結束時間：\" + endTime.strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    logging.critical(\"執行時間：\" + str((endTime - strTime).seconds) + \" 秒\")\n",
    "    logging.critical(\"輸出筆數：\" + str(len(df_1)) + \" 筆\")\n",
    "    logging.critical(\"爬網結束......\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取網址：http://law.fsc.gov.tw/law/NewsContent.aspx?id=7520\n",
      "爬取成功\n",
      "\n",
      "\n",
      "爬取網址：http://gazette.nat.gov.tw/egFront/detail.do?metaid=103631&log=detailLog\n",
      "爬取成功\n",
      "\n",
      "\n",
      "爬取網址：http://gazette.nat.gov.tw/egFront/detail.do?metaid=103590&log=detailLog\n",
      "爬取成功\n",
      "\n",
      "\n",
      "爬取網址：http://law.fsc.gov.tw/law/NewsContent.aspx?id=7516\n",
      "爬取成功\n",
      "\n",
      "\n",
      "爬取網址：http://law.fsc.gov.tw/law/NewsContent.aspx?id=7515\n",
      "爬取成功\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    url = \"http://law.fsc.gov.tw/law\"\n",
    "    main(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
